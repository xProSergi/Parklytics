import os
from pyspark.sql import SparkSession

# --- ConfiguraciÃ³n de entorno ---
os.environ["JAVA_HOME"] = "C:\\Program Files\\Eclipse Adoptium\\jdk-17.0.17.10-hotspot"
os.environ["SPARK_HOME"] = "C:\\dev\\spark\\spark-3.5.1-bin-hadoop3"
os.environ["HADOOP_HOME"] = "C:\\dev\\spark\\spark-3.5.1-bin-hadoop3"

# --- Crear SparkSession ---
spark = SparkSession.builder \
    .appName("Parklytics_ETL_Load_Test") \
    .master("local[*]") \
    .getOrCreate()

print("âœ… SparkSession creada correctamente.")
print(f"VersiÃ³n de Spark: {spark.version}\n")

# --- Ruta del dataset ---
data_path = "C:/Parklytics/src/processing/data/raw/data.csv"

# --- Lectura del CSV ---
df = spark.read.csv(data_path, header=True, inferSchema=True)

print("ðŸ“„ Datos cargados desde CSV:")
df.show()

print("ðŸ§± Esquema del DataFrame:")
df.printSchema()

# --- EstadÃ­sticas bÃ¡sicas ---
print("ðŸ“Š DescripciÃ³n estadÃ­stica:")
df.describe().show()

# --- Finalizar sesiÃ³n ---
spark.stop()
print("\nâœ… Proceso completado con Ã©xito.")
